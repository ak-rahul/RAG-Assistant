
<div align="center">
  <img src="docs/images/banner.png" alt="RAG Assistant" width="100%">
</div>


[![Python](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-UI-FF4B4B.svg)](https://streamlit.io/)
[![LangChain](https://img.shields.io/badge/LangChain-RAG-green.svg)](https://www.langchain.com/)
[![ChromaDB](https://img.shields.io/badge/Chroma-VectorDB-orange.svg)](https://www.trychroma.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)


A **Retrieval-Augmented Generation (RAG)** assistant for interactive document-based Question Answering.  
Upload PDFs, DOCX, TXT, Markdown, or JSON files, and interact with them via **Streamlit UI**, **CLI**, or **FastAPI API**.

---

## ğŸš€ Features

- ğŸ“‚ **Multi-format ingestion**: PDF, DOCX, TXT, MD, JSON  
- âœ‚ï¸ **Smart text splitting** with overlapping chunks  
- ğŸ§  **Vector embeddings** using SentenceTransformers  
- ğŸ—‚ **ChromaDB persistence** for long-term storage  
- ğŸ¤– **LLM providers**: Groq & OpenAI  
- ğŸ–¥ **Multiple interfaces**: Streamlit UI, CLI, FastAPI  
- ğŸ“ **Config-driven** via `config.yaml`  
- ğŸ” **Source citation** for transparency  

---
## âš¡ Quick Start (5 Minutes)

1. **Clone and Install**
```
git clone https://github.com/ak-rahul/RAG-Assistant.git
cd RAG-Assistant
pip install -r requirements.txt
```

2. **Set API Key**
```
echo "GROQ_API_KEY=your_key_here" > .env
```

ğŸ’¡ Get free key at: https://console.groq.com/

3. **Run the App**
```
python cli.py web
```

4. **Try the Sample**
- We included `data/sample.txt` to get started
- Ask: "What is RAG?" or "List the benefits"
- Upload your own documents and explore!

ğŸ“š More examples in [examples/](examples/)

---
## ğŸ“¸ Screenshots

![Upload Documents](docs/screenshots/upload-demo.png)
*Upload and process documents*

![Ask Questions](docs/screenshots/query-demo.png)
*Get answers with cited sources*

---

## ğŸ“‚ Project Structure

```bash
rag-assistant/
â”‚
â”œâ”€â”€ app.py                 # Streamlit UI
â”œâ”€â”€ cli.py                 # CLI entrypoint
â”œâ”€â”€ config.yaml            # Config file
â”œâ”€â”€ requirements.txt       # Dependencies
â”œâ”€â”€ scripts/               # Helper scripts
â”‚   â”œâ”€â”€ rag.sh 
â”‚   â””â”€â”€ rag.bat
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ server.py          # FastAPI app
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â””â”€â”€ rag_pipeline.py
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â””â”€â”€ chroma_handler.py
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â””â”€â”€ ingest.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ file_loader.py
â”‚       â””â”€â”€ text_splitter.py
â”‚
â”œâ”€â”€ data/                  # Uploaded docs
â”œâ”€â”€ logs/                  # Logs
â””â”€â”€ README.md
```

---

## âš™ï¸ Configuration

config.yaml controls everything:

```bash
data:
  source_dir: "./data"
  allowed_ext: [".pdf", ".docx", ".txt", ".md", ".json"]

vector_store:
  persist_directory: "./.chroma"
  embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
  top_k: 4

ingestion:
  chunk_size: 1200
  chunk_overlap: 150

llm:
  provider: groq
  model: llama-3.1-8b-instant 
  temperature: 0.2
  max_tokens: 512
```

ğŸ”‘ Environment overrides via .env: 

``` bash
GROQ_API_KEY=...
OPENAI_API_KEY=...
```
---

## ğŸ“‹ Prerequisites

- Python 3.9 or higher [attached_file:1][attached_file:2]
- Groq API key (free tier available at [console.groq.com](https://console.groq.com/))
- 2GB disk space for vector embeddings
- (Optional) OpenAI API key for alternative LLM

---

## âš™ï¸ Setup

### 1. Clone the Repository
```bash
git clone https://github.com/ak-rahul/rag-assistant.git
cd rag-assistant
```
### 2. Create Virtual Environment
```bash
python -m venv .venv
source .venv/bin/activate   # (Linux/Mac)
.venv\Scripts\activate      # (Windows)
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Configure Environment
Create a .env file in the project root:
```bash
GROQ_API_KEY=your_groq_api_key
OPENAI_API_KEY=your_openai_api_key   # optional
```

---

## ğŸ–¥ Usage

### Web UI

Launch the Streamlit UI:
```bash
python cli.py web
```

- Upload documents in the sidebar

- Ask questions in the chat interface

- Inspect DB stats or clear DB

### CLI

Ingest Documents
```bash
python cli.py ingest
```

Query
```bash
python cli.py query "What is Kali Linux?"
```

Run API Server
```bash
python cli.py serve
```

Show DB Stats
```bash
python cli.py stats
```

Clear DB
```bash
python cli.py clear
```

---

## ğŸ§© Architecture
```mermaid
flowchart TD
    A[User Uploads Files] --> B[File Loader]
    B --> C[Text Splitter]
    C --> D[ChromaDB Vector Store]
    D --> E[Retriever]
    E --> F[LLM via Groq/OpenAI]
    F --> G[Answer + Sources]
```
---

## ğŸ§¾ Example Workflow

- Upload a PDF in the Streamlit sidebar
- The file is chunked and ingested into ChromaDB
- Ask a question like:
  - "What is covered in Chapter 2 of the Kali Linux PDF?"
- The system retrieves relevant chunks â†’ sends them to the LLM â†’ returns an answer with sources

---

## ğŸ“Š Vector Store Management

- All documents are persisted in ChromaDB inside ./.chroma
- You can check stats (total docs, embeddings, metadata)
- Use Clear DB to reset your database

---

## â“ Troubleshooting

**GROQ_API_KEY not set**  : Create .env file with your API key
```
echo "GROQ_API_KEY=gsk_your_key" > .env
```

**ChromaDB errors**
```
python cli.py clear # Clear database
python cli.py ingest # Re-ingest documents
```

**No documents found**
- Ensure files are in `./data` folder
- Run `python cli.py ingest`

**Slow responses**
- Reduce `top_k` in `config.yaml` (4 â†’ 2)

**Need help?** [Open an issue](https://github.com/ak-rahul/RAG-Assistant/issues)


---

## ğŸ›  Tech Stack

- **Python 3.9+**  
- [Streamlit](https://streamlit.io/) â€“ Web UI  
- [LangChain](https://www.langchain.com/) â€“ RAG pipeline  
- [ChromaDB](https://www.trychroma.com/) â€“ Vector store  
- [Groq LLM](https://console.groq.com/) â€“ LLM provider  
- [OpenAI (optional)](https://platform.openai.com/) â€“ Alternative LLM provider  

---

## ğŸ“ License

This project is licensed under the [MIT License](LICENSE).
